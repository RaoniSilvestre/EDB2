# 1. ANALISE TEORICA

## 1.1 BUBBLESORT

### 1.1.1 VERS√ÉO ITERATIVA

Pseudoc√≥digo da vers√£o iterativa:

```
function bubbleSort(array) {
    n = tamanho do array
    para i de 0 at√© n-1 {
        para j de 0 at√© n-i-2 {
            se array[j] > array[j+1] {
                trocar array[j] com array[j+1]
            }
        }
    }
}
```
---

Note que o algoritmo foi implementado para um array de tamanho $n$, onde o loop externo √© executado $n-1$ vezes. O loop interno realiza suas compara√ß√µes com base no valor de $i$ no loop externo para evitar compara√ß√µes desnecess√°rias nas √∫ltimas posi√ß√µes do array, que j√° foram ordenadas em passagens anteriores.

Por exemplo, para um array de tamanho $n = 5$:

- Para $i = 0$, o loop interno √© executado 4 vezes, pois ele compara do primeiro at√© o pen√∫ltimo elemento, colocando o maior valor na √∫ltima posi√ß√£o.

- Para $i = 1$, o loop interno executa 3 vezes, pois o √∫ltimo elemento j√° est√° ordenado, e agora o segundo maior ser√° posicionado.

- Para $i = 2$, o loop interno executa 2 vezes, pois os dois √∫ltimos elementos est√£o ordenados.

- Para $i = 3$, o loop interno executa apenas 1 vez, pois os tr√™s √∫ltimos elementos j√° est√£o na posi√ß√£o correta.

Veja que somando o n√∫mero de itera√ß√µes para esse array, temos $(4+3+2+1) = 10$. Note que o n√∫mero de compara√ß√µes realizadas pelo loop interno em cada passagem do loop externo diminui gradualmente. Assim, em um array de tamanho $n$, temos $((n-1)+(n-2)+(n-3)...+1)$ compara√ß√µes. Ou seja, o tempo de execu√ß√£o do algoritmo pode ser representado da seguinte forma:

<br>

\begin{align*} 
T(n) &= \sum_{i=1}^{n-1} i \\
&= \frac{(1 + n - 1)(n-1)}{2} \\
&= \frac{n(n-1)}{2} \\
&= \frac{n^2 - n}{2} \\
\end{align*}

<br>

Portanto,

<br>

$$
T(n) = O(n^2)
$$

<br>

Como o algoritmo percorre o array com o mesmo n√∫mero de itera√ß√µes em todos os casos, o n√∫mero total de compara√ß√µes e, consequentemente, a complexidade de tempo permanece $n^2$ para os casos pior, m√©dio e melhor.
Podemos verificar isso calculando o limite de de $T(n)/n^2$, para n tendendo ao infinito:


\begin{align*}
\lim_{n \to \infty} \frac{\frac{n^2 - n}{n^2}}{n^2}
&= \frac{1}{2}\lim_{n \to \infty}\frac{n^2 - n}{n^2} \\
&= \frac{1}{2}\lim_{n \to \infty}1-\frac{1}{n} \\
&= \frac{1}{2}(\lim_{n \to \infty}1-\lim_{n \to \infty}\frac{1}{n}) \\
&= \frac{1}{2}(1 - 0)\\
&= \frac{1}{2} ‚àà ‚Ñù^*‚Çä
\end{align*}

Portanto,

$$
T(n) = Œò(n^2)
$$


### 1.1.2 VERS√ÉO RECURSIVA


Neste algoritmo de ordena√ß√£o, s√£o efetuadas compara√ß√µes entre os dados amarzenados em um array de tamanho n. Cada elemento de posi√ß√£o *i*  ser√° comparado com o elemento de posi√ß√£o *i* + 1, se o primeiro for maior (no caso de ordena√ß√£o crescente), os elemento trocam de posi√ß√µes. Ent√£o o algoritmo chama a si mesmo at√© a cole√ß√£o estar completamente ordenada.

```
function bubbleSort(array, tamanho) {
    se tamanho <= 1 {                      // Œò(1)
	retornar
    }
    para i de 0 at√© tamanho-1 {            // Œò(n - 1)
	    se array[i] > array[i+1] {
          trocar array[i] com array[i+1]
        }
    }
    bubbleSort( array, tamanho-1)         // T(n - 1)
}

```

---

Analisando a fun√ß√£o verifique que h√° um la√ßo que faz *n* - 1 compara√ß√µes. Al√©m da chamada recursiva com tamanho de entrada decrementado em 1, logo com tempo de execu√ß√£o representado por T(*n* - 1). Assim, temos a seguinte express√£o de recorr√™ncia:

<br>

$$
T(n) =
\begin{cases}
O(1),  \text{ se } n ‚â§ 1\\
T(n‚àí1)+ (n-1),  \text{ se } n > 1
\end{cases}
$$

<br>

#### 1.1.2.1 M√âTODO DA SUBSTITUI√á√ÉO

Considerando a recorr√™ncia acima, mostraremos que o algoritmo √© limitado por O(n¬≤).

Temos $T(n) \leq T(n-1) + n$, queremos mostrar que T(n) √© limitada superiormente por uma fun√ß√£o F(n) = cn¬≤, para algum c. Para isso usaremos indu√ß√£o.

###### Caso base:

$n = 1, T(1) = 1$

$$
T(n) \leq cn¬≤ \implies T(1) = 1 \leq c(1¬≤) \implies 1 \leq c
$$

###### Passo indutivo:

Hipotese: $T(k) \leq ck¬≤, (‚àÄk)[1 \leq k \leq n]$

\begin{align*}
T(n) \leq cn¬≤ &\implies T(n-1) + n \leq c(n-1)¬≤ + n \leq cn¬≤ \\
&\implies c(n¬≤ -2n + 1) + n \leq cn¬≤ \\
&\implies cn¬≤ -2nc + c + n \leq cn¬≤ \\
&\implies -2nc + c + n \leq 0 \\
&\implies c + n(1 - 2c) \leq 0
\end{align*}

Como `n` √© sempre um valor positivo e tende ao infinito, para que $c + n(1 - 2c) < 0$ seja verdade, precisamos que $1-2c < 0$.


\begin{align*}
1-2c < 0 &\implies 1 < 2c \\
&\implies c > 1/2 \\
\end{align*}

Logo, $T(n)$ √© $O(n¬≤)$.

#### 1.1.2.2 M√âTODO DA ITERA√á√ÉO

Considerando que o recorr√™ncia acima. Vamos expandir-la at√© encontrar o caso base.

<br>

Aplica-se *n* -1 sobre a f√≥rmula de T(n). E assim por diante.

<br>

\begin{align*}
T(n) &= T(n-1) + (n-1),\\
&= (T(n-2) + (n-2)) + (n-1)\\
&= (T(n-3) + (n-3) + (n-2)) + (n-1)\\
&= ...\\
T(n) &= T(n - k) +  \sum_{i=1}^{k} n - i
\end{align*}

<br>

Quando k = n -1, temos:

<br>

\begin{align*}
T(n) &= T(1) + (n‚àí1)+(n‚àí2)+‚Ä¶+1
\end{align*}

<br>

Que √© igual a:

<br>

\begin{align*}
T(n) &= T(1) + \frac{n(n-1)}{2}
\end{align*}

<br>

Aqui, T(1) representa o custo da fun√ß√£o no caso base. Podemos assumir que T(1) = O(1), j√° que n√£o h√° compara√ß√µes necess√°rias quando temos apenas um elemento.

Portanto, a complexidade total √©:

<br>

\begin{align*}
T(n) &= O(n^2)
\end{align*}

<br>


## 1.2 MERGESORT

### 1.2.1 VERS√ÉO ITERATIVA 

Pseudoc√≥digo da vers√£o iterativa:

```
function mergeSort(array, tamanho) {    
    i=1
    enquanto i √© menor que tamanho {
		esquerda = 0
		enquanto esquerda √© menor que tamanho-1 {
			meio = o menor entre (esquerda + i-1) e (tamanho-1)
			direita = o menor entre (esquerda + 2*i-1) e (tamanho-1)
			merge(array, esquerda, meio, direita)
		esquerda += 2*i
		}
    	i *= 2
    }
}

function merge(array, esquerda, meio, direita) {
    arrayEsquerdo = elementos de array[esquerda] at√© array[meio]
    arrayDireito = elementos de array[meio+1] at√© array[direita]

    posi√ß√£oAtual = esquerda

    enquanto esquerda n√£o estiver vazia e direita n√£o estiver vazia {
        se arrayEsquerdo[0] ‚â§ arrayDireito[0] {
            sobrescrever arrayEsquerdo[0] em array[posi√ß√£oAtual]
            remover arrayEsquerdo[0] de arrayEsquerdo
        }sen√£o{
            sobrescrever arrayDireito[0] em array[posi√ß√£oAtual]
            remover arrayDireito[0] de arrayDireito
        }
	posi√ß√£oAtual += 1
    }    

    enquanto arrayEsquerdo n√£o estiver vazio {
        adicionar arrayEsquerdo[0] a array[posi√ß√£oAtual]
	remover arrayEsquerdo[0] de arrayEsquerdo
	posi√ß√£oAtual += 1
    }
    
    Enquanto arrayDireito n√£o estiver vazio {
        adicionar arrayDireito[0] a array[posi√ß√£oAtual]
	remover arrayDireito[0] de arrayDireito
	posi√ß√£oAtual += 1
    }

}
```

---


Vamos come√ßar analisando a fun√ß√£o merge, que faz a intercala√ß√£o de dois arrays, pecorrendo todas as posi√ß√µes dos vetores, com custo de n = m<sub>1</sub> + m<sub>2</sub>, onde m<sub>1</sub> e m<sub>2</sub> s√£o os tamanhos do vetor 1 e vetor 2, respectivamente. Logo a complexidade de tempo da fun√ß√£o merge √© $T(n) = O(n)$.

<br>

No mergeSort, h√° dois loops, o externo controla a divis√£o do array em sublistas de tamanho crescente, que v√£o sendo mescladas √† medida que o valor de $i$ dobra a cada itera√ß√£o. O loop interno percorre o array, criando pares de sublistas para serem mescladas em cada itera√ß√£o do loop externo. Como i dobra a cada itera√ß√£o, o n√∫mero de vezes que o loop externo executa √© $O(\log_2n)$, pois a cada itera√ß√£o o tamanho das sublistas dobra at√© que elas cubram o array inteiro.
Portanto, para cada valor de i, o loop interno executa O(n) opera√ß√µes, pois ele percorre o array inteiro dividindo-o em sublistas de tamanho i e realizando uma chamada para merge para cada par de sublistas.

Portanto, a complexidade total √©

<br>

$$
T(n) = O(n\log n)
$$

<br>

Mesmo que o array esteja ordenado, o algoritmo ainda precisa percorrer todos os n√≠veis de divis√£o e realizar opera√ß√µes de mesclagem, logo todos os casos (melhor, m√©dio e pior) t√™m a mesma complexidade


### 1.2.2 VERS√ÉO RECURSIVA

Neste algoritmo de ordena√ß√£o, a sequ√™ncia de *n* elementos √© dividida em duas subs√™nqueicias de *n*/2 elementos e n√£o ordenadas recursivamente. Ent√£o as subsequ√™ncia s√£o intercaladas para produzir uma solu√ß√£o.

```
function mergeSort(array, esquerda, direita ) {
    se esquerda >= direita {                          // Œò(1)
    	retornar
    }
    
    meio = esquerda + (direita - esquerda) / 2       // Œò(1)

    mergeSort(array, esquerda, meio)
    mergeSort(array, meio+1, direita)
    
    merge(array, esquerda, meio, direita)
}
```

Antes de calcular o tempo de execu√ß√£o do mergeSort, devemos analisar a fun√ß√£o merge.

```
function merge(array, esquerda, meio, direita) {

    arrayEsquerdo = elementos de array[esquerda] at√© array[meio]
    arrayDireito = elementos de array[meio+1] at√© array[direita]


    posi√ß√£oAtual = esquerda

    Enquanto esquerda n√£o estiver vazia e direita n√£o estiver vazia { //O(n)
        se arrayEsquerdo[0] ‚â§ arrayDireito[0] {
            sobrescrever arrayEsquerdo[0] em array[posi√ß√£oAtual]
            remover arrayEsquerdo[0] de arrayEsquerdo
        }sen√£o{
            sobrescrever arrayDireito[0] em array[posi√ß√£oAtual]
            remover arrayDireito[0] de arrayDireito
        }
	posi√ß√£oAtual += 1
    }
    

    Enquanto arrayEsquerdo n√£o estiver vazio {
      adicionar arrayEsquerdo[0] a array[posi√ß√£oAtual]
      remover arrayEsquerdo[0] de arrayEsquerdo
      posi√ß√£oAtual += 1
    }
    
    Enquanto arrayDireito n√£o estiver vazio {
      adicionar arrayDireito[0] a array[posi√ß√£oAtual]
      remover arrayDireito[0] de arrayDireito
      posi√ß√£oAtual += 1
    }
}
```

A fun√ß√£o merge faz a intercala√ß√£o de dois arrays, pecorrendo todas as posi√ß√µes dos vetores, com custo de n = m<sub>1</sub> + m<sub>2</sub>, onde m<sub>1</sub> e m<sub>2</sub> s√£o os tamanhos do vetor 1 e vetor 2, respectivamente.

Assim, verifica-se que nosso m√©todo principal faz duas chamadas recursivas com tamanhos de entrada divididos pela metade, logo com tempo de execu√ß√£o representado por $T(\frac{n}{2})$ cada uma.

Portanto, a complexidade total √©:

<br>

$$
T(n) =
\begin{cases}
O(1),  \text{ se } n ‚â§ 1\\
2T(\frac{n}{2}) + n,  \text{ se } n > 1
\end{cases}
$$

<br>

#### 1.2.2.1 M√âTODO DA SUBSTITUI√á√ÉO

#### 1.2.2.2 M√âTODO DA ITERA√á√ÉO

Considerando que o recorr√™ncia acima. Vamos expandir-la at√© encontrar o caso base.

<br>

Aplica-se *n*/2 sobre a f√≥rmula de T(n). E assim por diante.

<br>

\begin{align*}
T(n) &= 2T(\frac{n}{2}) + n,\\
&= 2(2T(\frac{n}{4}) + \frac{n}{2}) + n\\
&= 2(2(2T(\frac{n}{8}) + \frac{n}{4}) + \frac{n}{2}) + n\\
&= ...\\
T(n) &= 2^kT(\frac{n}{2^k}) + k.n
\end{align*}

<br>

Vamos encontrar para qual valor de k, $\frac{n}{2^k} = 1$.

<br>

\begin{align*}
\frac{n}{2^k} = 1 &\implies n = 2^k\\
&\implies k = log_{2} n
\end{align*}

<br>

Aplicando na recorr√™ncia:

<br>

\begin{align*}
T(n) &= 2^{log_{2} n}T(\frac{n}{2^{log_{2} n}}) + n.{log_{2} n} \\
&= n.T(\frac{n}{n}) + n.{log_{2} n} \\
&= n + n.log_{2} n
\end{align*}\\

<br>

Portanto, a complexidade total √©:

<br>

\begin{align*}
T(n) &= O(n\log_{}n)
\end{align*}

<br>

#### 1.2.2.3 M√âTODO DA ARVORE DE RECURS√ÉO

A √°rvore de chamadas do MergeSort come√ßa com um n√≥ raiz que representa o problema original de tamanho $\frac{n}{2}$. Em cada n√≠vel da √°rvore, o array √© dividido em duas sublistas de tamanhos iguais, resultando em duas chamadas recursivas para problemas de tamanho . Cada uma dessas chamadas recursivas gera mais dois n√≥s, e assim por diante.

Esse processo de divis√£o continua at√© atingirmos o caso base, em que o tamanho do array √© $n=1$, como mostrado a seguir:

```
           T(n)------------------n
          /    \
         /      \
  T(n/2)          T(n/2)---------2n/2 = n
  /   \           /   \
 /     \         /     \
T(n/4) T(n/4) T(n/4) T(n/4)-----4n/4 = n
 ...     ...   ...     ...
T(1)
```

A altura da √°rvore √© o n√∫mero de n√≠veis at√© chegar ao caso base. Na primeira chamada recursiva, temos o termo $T(\frac{n}{2})$, em seguida $T(\frac{n}{2^2})$, $T(\frac{n}{2^3})$,... at√© $T(\frac{n}{2^h}) = T(1)$, onde h corresponde a altura da √°rvore.

Calculando h:

<br>

\begin{align*}
T(\frac{n}{2^h}) = T(1) &\implies \frac{n}{2^h} = 1\\
&\implies n = 2^h\\
&\implies \log_{2}n = \log_{2}2^h\\
&\implies h = \log_{2}n\\
\end{align*}

<br>

Como o tempo de execu√ß√£o do algoritmo corresponde corresponde a soma dos passos de todos os n√≠veis, temos:

<br>

\begin{align*}
T(n) &= \sum_{i=0}^{h} n\\
&= n\sum_{i=0}^{h} 1\\
&= n(\log_{2}n + 1)\\
&= O(n\log_{}n)\\
\end{align*}

<br>

#### 1.2.2.4 M√âTODO DO TEOREMA MESTRE

!TODO

## 1.3 QUICKSORT

### 1.3.1 VERS√ÉO ITERATIVA

Pseudoc√≥digo da vers√£o iterativa:

```
function quickSort(array, tamanho){
    pilha = pilha vazia

    empilhar(pilha,0)
    empilhar(pilha, tamanho-1)
    enquanto pilha n√£o estiver vazia {
		direita = topo da pilha
		retirar topo da pilha
		esquerda = topo da pilha
		retirar topo da pilha

		pivoIndex = particionar(array, esquerda, direita)

		se pivoIndex - 1 > esquerda {
			empilhar(pilha, esquerda)
			empilhar(pilha, pivoIndex-1)
		}
		se pivoIndex + 1 < direita {
			empilhar(pilha, pivoIndex+1)
			empilhar(pilha, direita)
		}
    }
}

function particionar(array, esquerda, direita) {
    pivo = array[direita]
    i = esquerda - 1
    para j de esquerda at√© direita - 1 {
        se array[j] <= pivo {
            i += 1
            trocar array[i] com array[j]
        }
    }
    trocar array[i + 1] com array[direita]
    retornar i + 1
}
```

##### Pior caso

Quando o array est√° ordenado e o piv√¥ escolhido √© sempre o maior ou menor elemento, a fun√ß√£o particionar divide o array em uma sublista vazia e uma sublista com n‚àí1 elementos. Assim, Cada chamada para particionar tem um custo de Œò(n) no pior caso, pois √© necess√°rio percorrer o array para posicionar o piv√¥ corretamente.

A fun√ß√£o quickSort usa uma pilha iterativa que armazena os limites das parti√ß√µes ainda n√£o processadas. No pior caso, essa pilha armazena at√© Œò(n) elementos ao longo da execu√ß√£o, como ele precisa ser realizado $n$ vezes, a complexidade de tempo total √©:

<br>

$$
T(n) = O(n^2)
$$

<br>

##### Melhor caso

No melhor caso, o piv√¥ divide o array em duas partes de tamanho $n/2$, na primeira chamada, $n/4$, na segunda chamada, $n/8$, na terceira chamada,..., $n/2^i$, para i chamadas. Isso resulta em custo $\log_{}n$.
Como partionamento tem custo $Œò(n)$, ent√£o o custo total √©:

<br>

$$
T(n) = Œ©(n\log_{}n)
$$

<br>

### 1.3.2 VERS√ÉO RECURSIVA

O QuickSort √© um algoritmo de ordena√ß√£o que funciona dividindo repetidamente o array em duas partes menores at√© que cada subarray tenha no m√°ximo um elemento.

```
function quickSort(array, esquerda, direita) {
    se esquerda >= direita {
	retornar
    }

    pivoIndex = particionar(array, esquerda, direita)

        
    quickSort(array, esquerda, pivoIndex - 1)
    quickSort(array, pivoIndex + 1, direita)
}
```

A fun√ß√£o *particionar* percorre o array usando o √≠ndice j, comparando cada elemento array[j] com o piv√¥. Se o valor de array[j] √© menor ou igual ao piv√¥, ele √© trocado com o elemento na posi√ß√£o i, que mant√©m a posi√ß√£o de divis√£o entre os elementos menores e maiores que o piv√¥. Ao final do loop, todos os elementos √† esquerda de i s√£o menores ou iguais ao piv√¥, e todos os elementos √† direita s√£o maiores.

```
function particionar(array, esquerda, direita) {

    pivo = array[direita]
    i = esquerda - 1

    para j de esquerda at√© direita - 1 {
        se array[j] ‚â§ pivo {
            i += 1
            trocar array[i] com array[j]
        }
    }

    trocar array[i + 1] com array[direita]
    retornar i + 1
}
```

---

No pior caso do QuickSort, a fun√ß√£o particionar divide o array de forma altamente desbalanceada, resultando em uma sublista vazia e uma sublista com n‚àí1 elementos. Isso acontece, por exemplo, quando o array j√° est√° ordenado e o piv√¥ escolhido √© o maior ou menor elemento. Como a fun√ß√£o particionar tem complexidade Œò(1), a recorrencia pode ser expressa como:

<br>

$$
T(n) = T(n-1) + n
$$

<br>

No melhor caso do QuickSort, cada chamada de particionamento divide o array em duas sublistas de tamanho aproximadamente n/2. Isso resulta na seguinte recorr√™ncia:

$$
T(n) = 2T(\frac{n}{2}) + n
$$

Onde $2T(\frac{n}{2})$ representa o custo das duas chamadas recursivas em subarrays de tamanho $\frac{n}{2}$.

Como visto na analise do Merge Sort, tal recorr√™ncia tem custo:

<br>

$$
T(n) = O(n * \log{2})
$$

<br>

#### 1.3.2.1 M√âTODO DA SUBSTITUI√á√ÉO

#### 1.3.2.2 M√âTODO DA ITERA√á√ÉO

Considerando que o recorr√™ncia acima. Vamos expandir-la at√© encontrar o caso base.

<br>

Aplica-se *n* -1 sobre a f√≥rmula de T(n). E assim por diante.

<br>

\begin{align*}
T(n) &= T(n-1) + n,\\
&= (T(n-2) + n) + n\\
&= ((T(n - 3) + n)+ n) + n\\
&= ...\\
T(n) &= T(n-k) + k.n
\end{align*}

<br>

Note que,

<br>

\begin{align*}
n - k = 1 &\implies k = n - 1\\
\end{align*}

<br>

Aplicando na recorr√™ncia:

<br>

\begin{align*}
T(n) &= T(n-k) + k.n,\\
&= T(n-(n-1)) + n(n-1)\\
&= T(1) + n^2 - n\\
&= 1 + n^2 - n
\end{align*}

<br>

Portanto, a complexidade no pior caso √©:

<br>

\begin{align*}
T(n) &= O(n^2)
\end{align*}

<br>

#### 1.3.2.3 M√âTODO DA ARVORE DE RECURS√ÉO

#### 1.3.2.4 M√âTODO DO TEOREMA MESTRE




# 2. IMPLEMENTA√á√ïES

As implementa√ß√µes dos c√≥digos foram feitas na maior quantidade de linguagens poss√≠vel a tarefa: 3.

- Uma linguagem para idadeRep e idadeRep2: Lua
- Uma para a busca bin√°ria: C
- Uma para os algoritmos de ordena√ß√£o: Rust

A escolha foi feita pela familiaridade dos integrantes do grupo com tais linguagens, e porque parecia mais divertido utilizar mais linguagens visto que a escolha √© livre. Isso de forma que a escolha das linguagens n√£o prejudique os resultados, obviamente.

## 2.1 AMBIENTE COMPUTACIONAL UTILIZADO

Todas as implementa√ß√µes foram executadas e cronometradas no mesmo ambiente computacional, para uma compara√ß√£o justa e adequada entre as diferentes formas de implementa√ß√£o das fun√ß√µes citadas no roteiro avaliativo.
	
O ambiente computacional consiste em uma m√°quina virtual no servidor pessoal de um dos integrantes do grupo. A m√°quina virtual possui dois n√∫cleos do processador, 2GB de mem√≥ria RAM e 32GB de disco r√≠gido √† sua disposi√ß√£o, rodando uma distribui√ß√£o Linux conhecida como Arch Linux, com uma instala√ß√£o m√≠nima, contendo somente os servi√ßos necess√°rios para funcionamento do sistema operacional e o servi√ßo de servidor SSH, para conex√£o remota com a m√°quina. 
	
Todas as implementa√ß√µes foram cronometradas utilizando os m√©todos apropriados para cada linguagem de programa√ß√£o e seus respectivos tempos de execu√ß√£o foram armazenados em um arquivo final, para serem analisados no presente relat√≥rio.

## 2.2 FUN√á√ïES ITERATIVAS - IDADEREP e IDADEREP2

Essas fun√ß√µes foram implementadas na linguagem de programa√ß√£o lua, devido a um integrante que gosta muito de lua!

### 2.2.1 IdadeRep

A fun√ß√£o idadeRep percorre a lista duas vezes:

O primeiro for percorre a lista para encontrar o menor valor (caso o menor valor seja menor que 200), com complexidade de O(n), onde n √© o tamanho da lista.
O segundo for verifica se o menor valor est√° presente na lista, tamb√©m com complexidade O(n), com a verifica√ß√£o podendo ser interrompida antecipadamente caso encontr√°-lo.
Portanto, a complexidade de idadeRep √© O(2n), ou seja, linear.

### 2.2.2 IdadeRep2

Por outro lado, a fun√ß√£o idadeRep2 ordena a lista e faz uma compara√ß√£o:

A chamada a table.sort() ordena a lista com complexidade O(n log n).
Em seguida, h√° uma compara√ß√£o entre os dois primeiros elementos, uma opera√ß√£o de custo O(1).
Dessa forma, a complexidade de idadeRep2 √© O(n log n).

### 2.2.3 An√°lise de Resultados

A tabela a seguir mostra como as fun√ß√µes idadeRep e idadeRep2 se comportaram com entrada de cem, mil e um milh√£o.

| Tamanho | IdadeRep | IdadeRep2 |
|---|---|---|
| 100 | 0.017 ms | 0.071 ms | 
| 1,000 | 0.063 ms | 0.891 ms | 
| 1,000,000 | 48.32 ms | 1688.003 ms| 

<br>
Com 100 elementos de entrada, a fun√ß√£o idadeRep2 j√° apresentou um tempo de execu√ß√£o maior em rela√ß√£o a idadeRep, apesar da diferen√ßa minuscula de apenas 0,054 milisegundos. Algo interessante de se apontar, √© que a fun√ß√£o de ordena√ß√£o em lua, √© feita com base em C, que √© uma linguagem objetivamente mais r√°pida. Por√©m, devido a complexidade da fun√ß√£o de ordena√ß√£o ser maior, Idaderep ainda √© mais r√°pida que Idaderep2.

<br>
√Ä medida que a lista cresce, a diferen√ßa fica mais evidente. A idadeRep2 come√ßa a ser notavelmente mais lenta porque a ordena√ß√£o envolve mais opera√ß√µes do que o simples percurso linear de idadeRep.

<br>

Com 1.000.000 de elementos, a diferen√ßa √© alarmante. A fun√ß√£o idadeRep (linear) tem um tempo de execu√ß√£o muito menor que idadeRep2, que √© O(n log n). Isso reflete a diferen√ßa assint√≥tica entre O(n) e O(n log n). A ordena√ß√£o vai se tornando muito mais custosa √† medida que o tamanho da entrada aumenta.
<br>

Portanto, idadeRep √© mais eficiente assintoticamente, como evidenciado pelos tempos de execu√ß√£o menores.

## 2.3 FUN√á√ïES RECURSIVAS - BUSCABINARIA E BBINREC

### 2.3.1 BuscaBinaria

A fun√ß√£o buscaBinaria implementa o algoritmo de busca bin√°ria, em sua vers√£o iterativa, para encontrar a posi√ß√£o de um elemento (chave) em um vetor ordenado. Inicialmente, os √≠ndices da esquerda (esq) e direita (dir) s√£o definidos como os extremos do vetor. Em cada itera√ß√£o do loop while, o √≠ndice do meio (m) √© calculado. Se o valor no √≠ndice m for menor que a chave, o √≠ndice esquerdo √© movido para m + 1; caso contr√°rio, o √≠ndice direito √© ajustado para m. Esse processo continua at√© que esq e dir se encontrem. Quando o loop termina, a fun√ß√£o verifica se o elemento encontrado √© a chave buscada; se for, retorna o √≠ndice esq, caso contr√°rio, retorna -1. 

Essa implementa√ß√£o percorre a lista em tempo O(log n), onde n √© o tamanho do vetor, j√° que a cada itera√ß√£o o intervalo de busca √© reduzido pela metade. Essa implementa√ß√£o √© eficiente para vetores grandes.

### 2.3.2 bBinRec

A fun√ß√£o bBinRec implementa a busca bin√°ria de forma recursiva para encontrar a posi√ß√£o de uma chave em um vetor. A fun√ß√£o come√ßa verificando se o intervalo de busca est√° esgotado: se esq for maior ou igual a dir e o elemento no √≠ndice esq n√£o for igual √† chave, a fun√ß√£o retorna -1, indicando que a chave n√£o foi encontrada. Em seguida, calcula o √≠ndice do meio (m) como o ponto m√©dio entre esq e dir. Se o elemento no √≠ndice m for igual √† chave, m √© retornado como o √≠ndice onde a chave foi encontrada.

Caso contr√°rio, a fun√ß√£o realiza uma chamada recursiva: se o valor em vetor[m] for maior que a chave, chama-se bBinRec para o subvetor da esquerda (esq at√© m - 1); se for menor, a chamada √© feita para o subvetor da direita (m + 1 at√© dir). Esse processo de divis√£o continua at√© que a chave seja encontrada ou que o intervalo de busca se esgote.

A complexidade da fun√ß√£o bBinRec √© O(log n), onde n √© o n√∫mero de elementos no vetor, pois a cada chamada recursiva o intervalo de busca √© reduzido pela metade.

### 2.3.3 An√°lise de Resultados

A tabela a seguir mostra os resultados de tempo de execu√ß√£o das fun√ß√µes buscaBinaria e bBinRec com vetores de 100, 1000 e 10000 elementos.

| Tamanho | buscaBinaria | bBinRec |
|:-:|:-:|:-:|
| 100 | 5Œºs | 4Œºs |
| 1000 | 6Œºs | 4Œºs |
| 10000 | 7Œºs | 4Œºs |

√â poss√≠vel perceber nos resultados apresentados acima que a fun√ß√£o bBinRec √© consistentemente mais r√°pida que buscaBinaria para os tr√™s tamanhos de vetor testados (100, 1000 e 10000 elementos). Enquanto buscaBinaria apresenta um aumento gradual no tempo de execu√ß√£o conforme o tamanho do vetor cresce (de 5Œºs para 7Œºs), bBinRec mant√©m um tempo constante de 4Œºs para todos os tamanhos. Essa diferen√ßa pode ser atribu√≠da √† natureza iterativa de buscaBinaria, que, embora eficiente, envolve verifica√ß√µes adicionais a cada itera√ß√£o. Em contrapartida, a recurs√£o de bBinRec permite uma elimina√ß√£o direta de metade do vetor em cada chamada sem repetir verifica√ß√µes, o que resulta em uma leve vantagem de desempenho. 

Entretanto, a recurs√£o pode consumir mais mem√≥ria de pilha, o que, para vetores muito grandes, pode se tornar uma limita√ß√£o. A partir desses resultados, pode-se concluir que bBinRec √© mais eficiente em termos de tempo, especialmente para vetores de tamanho moderado, embora a efici√™ncia de mem√≥ria de buscaBinaria possa ser prefer√≠vel em casos espec√≠ficos onde o uso de pilha √© uma preocupa√ß√£o.

## 2.4 ALGORITMOS DE ORDENA√á√ÉO - BUBBLESORT, QUICKSORT E MERGESORT

### 2.4.1 Bubblesort

#### 2.4.1.1 Vers√£o Iterativa

A fun√ß√£o iterative_bubble_sort implementa o algoritmo de ordena√ß√£o bolha (ou bubble sort) de forma iterativa em Rust. A fun√ß√£o primeiro calcula o tamanho do vetor (len), ent√£o utiliza dois loops aninhados para percorrer o vetor e ordenar seus elementos em ordem crescente. No loop externo, que controla o n√∫mero de passagens, o √≠ndice i representa a quantidade de elementos j√° ordenados ao final do vetor. O loop interno, controlado por j, percorre os elementos n√£o ordenados e compara cada par adjacente (array[j] e array[j + 1]). Caso array[j] seja maior que array[j + 1], os dois elementos s√£o trocados de lugar com array.swap(j, j + 1). Esse processo √© repetido at√© que o vetor esteja completamente ordenado.

A complexidade de tempo do iterative_bubble_sort √© O(n¬≤) no pior e no caso m√©dio, onde n √© o n√∫mero de elementos no vetor. Isso ocorre porque, para cada elemento, o algoritmo potencialmente percorre todo o vetor novamente, resultando em ùëõ √ó ùëõ compara√ß√µes. A complexidade no melhor caso √© O(n), quando o vetor j√° est√° ordenado, pois o algoritmo pode ser otimizado para detectar que n√£o h√° necessidade de trocas e parar antes.

#### 2.4.1.2 Vers√£o Recursiva

A fun√ß√£o recursive_bubble_sort implementa o algoritmo de ordena√ß√£o bolha (ou bubble sort) de maneira recursiva em Rust. A fun√ß√£o recursive_bubble_sort inicia a chamada √† fun√ß√£o auxiliar recursive_bubble, que aplica o algoritmo de ordena√ß√£o bolha recursivamente. Na recursive_bubble, a fun√ß√£o primeiro verifica se o tamanho do vetor (n) √© 1, caso em que a ordena√ß√£o est√° completa e a recurs√£o termina. Caso contr√°rio, recursive_bubble realiza uma passagem de ordena√ß√£o chamando a fun√ß√£o bubble_pass, que compara e troca elementos adjacentes recursivamente ao longo do vetor. Ap√≥s essa passagem, recursive_bubble √© chamada novamente com n - 1, reduzindo gradativamente o tamanho do vetor a ser ordenado at√© que todos os elementos estejam na posi√ß√£o correta.

A complexidade de tempo dessa implementa√ß√£o √© O(n¬≤) no pior e no caso m√©dio, onde n √© o n√∫mero de elementos no vetor, semelhante ao bubble sort iterativo. Isso ocorre porque a fun√ß√£o precisa realizar n passagens, cada uma envolvendo at√© n compara√ß√µes recursivas em bubble_pass. No melhor caso, onde o vetor j√° est√° ordenado, a fun√ß√£o ainda possui complexidade O(n¬≤), pois a implementa√ß√£o atual n√£o detecta se o vetor j√° est√° ordenado e sempre executa as passagens completas. A implementa√ß√£o recursiva ocupa mais espa√ßo na pilha devido √†s chamadas recursivas, o que pode ser uma limita√ß√£o em casos de vetores muito grandes.

#### 2.4.1.3 An√°lise de Resultados

A tabela a seguir mostra os resultados de tempo de execu√ß√£o das implementa√ß√µes da ordena√ß√£o de bolha (bubblesort) iterativa e recursiva com vetores de 1000, 10000 e 100000 elementos.

| Tamanho | Bubblesort Iterativo | Bubblesort Recursivo |
|:---:|:---:|:---:|
| 1000 | 3.20ms | 3.67ms |
| 10000 | 338.42ms | 360.27ms |
| 100000 | 34.23s | 35.83s |

Ambos os algoritmos exibem tempos de execu√ß√£o semelhantes em cada tamanho de vetor, mas a implementa√ß√£o iterativa √© consistentemente um pouco mais r√°pida. Com 1000 elementos, o bubble sort iterativo executa em 3.20 ms, enquanto o recursivo demora 3.67 ms. √Ä medida que o tamanho do vetor aumenta, essa diferen√ßa se torna mais significativa: com 10000 elementos, o iterativo leva 338.42 ms contra 360.27 ms para o recursivo, e com 100000 elementos, o iterativo demora 34.23 s, enquanto o recursivo leva 35.83 s.

Essa diferen√ßa ocorre porque o bubble sort recursivo tem o mesmo comportamento de tempo assint√≥tico (O(n¬≤)) que a vers√£o iterativa, mas carrega o custo adicional de m√∫ltiplas chamadas recursivas, que consomem mais mem√≥ria de pilha e exigem mais tempo para o gerenciamento das chamadas de fun√ß√£o. A recurs√£o, portanto, se torna um fator de sobrecarga crescente conforme o tamanho do vetor aumenta.

### 2.4.2 Quicksort

#### 2.4.2.1 Vers√£o Iterativa

A fun√ß√£o iterative_quick_sort implementa o algoritmo de ordena√ß√£o r√°pida (ou quick sort) de forma iterativa em Rust. Em vez de usar chamadas recursivas, esta implementa√ß√£o utiliza uma pilha (stack) para armazenar os √≠ndices das sublistas a serem ordenadas. Inicialmente, os √≠ndices do in√≠cio (0) e do fim (tamanho - 1) do vetor s√£o empilhados. Em seguida, a fun√ß√£o entra em um loop while, onde o limite direito (right) e o limite esquerdo (left) das sublistas s√£o retirados da pilha. A fun√ß√£o particionar √© chamada para particionar o vetor, colocando elementos menores que o piv√¥ √† esquerda e maiores √† direita, retornando o √≠ndice do piv√¥ final.

Ap√≥s a parti√ß√£o, a fun√ß√£o verifica se h√° sublistas √† esquerda e √† direita do piv√¥ que precisam ser ordenadas. Se houver, seus √≠ndices s√£o empilhados para serem processados em itera√ß√µes subsequentes. Esse processo continua at√© que a pilha esteja vazia, indicando que o vetor est√° totalmente ordenado.

A complexidade de tempo do iterative_quick_sort √© O(n log n) no caso m√©dio e O(n¬≤) no pior caso, onde n √© o n√∫mero de elementos no vetor. A efici√™ncia de O(n log n) no caso m√©dio ocorre porque a fun√ß√£o divide repetidamente o vetor ao meio, ordenando cada metade de forma independente. No entanto, no pior caso (por exemplo, se o vetor j√° estiver quase ordenado ou se sempre for escolhido um piv√¥ ineficiente), a complexidade pode degradar para O(n¬≤).

#### 2.4.2.2 Vers√£o Recursiva

A fun√ß√£o recursive_quick_sort implementa o algoritmo de ordena√ß√£o r√°pida (ou quick sort) de maneira recursiva em Rust. A fun√ß√£o inicializa a ordena√ß√£o chamando a fun√ß√£o sorting, que √© respons√°vel por particionar o vetor e chamar-se recursivamente em cada sublista. Na sorting, √© verificado se o √≠ndice esquerdo (left) √© menor que o direito (right). Se for, a fun√ß√£o partition √© chamada para reorganizar o vetor de forma que todos os elementos menores que o piv√¥ fiquem √† esquerda e os maiores √† direita, retornando o √≠ndice final do piv√¥ (index_pivot). A sorting ent√£o chama-se recursivamente para ordenar as sublistas √† esquerda e √† direita do piv√¥.

A fun√ß√£o partition utiliza o primeiro elemento da sublista como piv√¥ e faz compara√ß√µes a partir dos extremos (left e right) at√© que ambos se cruzem. Durante esse processo, ela compara os elementos com o piv√¥ e realiza trocas, ajustando os limites left e right conforme necess√°rio. Quando o loop termina, o piv√¥ √© trocado com o elemento em right, completando a parti√ß√£o.

A complexidade de tempo da recursive_quick_sort √© O(n log n) no caso m√©dio e O(n¬≤) no pior caso, onde n √© o n√∫mero de elementos, similar √† sua implementa√ß√£o iterativa. O caso m√©dio ocorre quando o vetor √© dividido aproximadamente ao meio em cada parti√ß√£o, enquanto o pior caso ocorre quando o piv√¥ escolhido resulta em parti√ß√µes muito desbalanceadas, como em vetores quase ordenados.

#### 2.4.2.3 An√°lise de Resultados

A tabela a seguir mostra os resultados de tempo de execu√ß√£o das implementa√ß√µes da ordena√ß√£o r√°pida (quicksort) iterativa e recursiva com vetores de 1000, 10000 e 100000 elementos.

| Tamanho | Quicksort Iterativo | Quicksort Recursivo |
|:---:|:---:|:---:|
| 1000 | 147.87¬µs | 149.96¬µs |
| 10000 | 1.21ms | 1.77ms |
| 100000 | 20.38ms | 55.42ms |

Em todas as inst√¢ncias, o quicksort iterativo apresenta tempos de execu√ß√£o mais r√°pidos do que a vers√£o recursiva, e essa diferen√ßa se torna mais pronunciada √† medida que o tamanho do vetor aumenta. Para um vetor de 1000 elementos, a diferen√ßa √© pequena: o quicksort iterativo leva 147.87 ¬µs, enquanto o recursivo leva 149.96 ¬µs. No entanto, com 10000 elementos, o iterativo demora 1.21 ms, enquanto o recursivo leva 1.77 ms. Entretanto, para 100000 elementos, a diferen√ßa aumenta de forma significativa, com o quicksort iterativo executando em 20.38 ms e o recursivo demorando 55.42 ms. A diferen√ßa nos tempos de execu√ß√£o com o vetor de 100000 elementos indica algum tipo de inefici√™ncia ligada ao maior custo espacial da implementa√ß√£o recursiva.

Esses resultados indicam que a implementa√ß√£o iterativa √© mais eficiente, principalmente para vetores maiores, devido ao menor custo de gerenciamento de chamadas de fun√ß√£o e uso de mem√≥ria. O quicksort recursivo, embora seja uma abordagem tradicional e intuitiva para esse algoritmo, acumula sobrecarga de mem√≥ria de pilha devido √†s sucessivas chamadas recursivas, o que leva a um desempenho inferior conforme o vetor cresce.

#### 2.4.3 Mergesort

#### 2.4.3.1 Vers√£o Iterativa

A fun√ß√£o iterative_merge_sort implementa o algoritmo de ordena√ß√£o por mesclagem (merge sort) de maneira iterativa em Rust. A fun√ß√£o come√ßa chamando a fun√ß√£o merge_sorting, que divide e ordena o vetor em peda√ßos de tamanho crescente, substituindo o vetor original pelo resultado ordenado. Em merge_sorting, o vetor √© particionado em sublistas de tamanho i, que dobra em cada itera√ß√£o do while externo, permitindo que peda√ßos ordenados sejam mesclados em intervalos crescentes at√© que todo o vetor seja ordenado. Dentro do loop interno, a fun√ß√£o identifica os limites mid e right_arr de cada sublista a ser mesclada e chama a fun√ß√£o auxiliar merge para combinar as duas partes.

A fun√ß√£o merge recebe duas sublistas (arr_1 e arr_2) e as combina em uma √∫nica lista ordenada (result). Ela usa dois √≠ndices (i e j) para percorrer ambas as sublistas, adicionando os elementos menores a result em cada etapa at√© que todos os elementos de uma das listas sejam esgotados. Os elementos restantes de qualquer uma das sublistas s√£o ent√£o adicionados a result.

A complexidade de tempo do iterative_merge_sort √© O(n log n) tanto no caso m√©dio quanto no pior caso, onde n √© o n√∫mero de elementos no vetor. Isso ocorre porque o algoritmo divide o vetor em duas metades repetidamente e, em seguida, mescla as metades ordenadas, cada etapa exigindo O(n) opera√ß√µes.

#### 2.4.3.2 Vers√£o Recursiva

A fun√ß√£o recursive_merge_sort implementa o algoritmo de ordena√ß√£o por mesclagem (merge sort) de forma recursiva em Rust. A fun√ß√£o principal chama merge_sorting, que divide recursivamente o vetor em duas metades at√© que cada sublista contenha apenas um elemento. A cada divis√£o, merge_sorting calcula o √≠ndice central (mid), particiona o vetor em duas sublistas (left_arr e right_arr), e ent√£o chama-se recursivamente para ordenar cada uma dessas metades. Em seguida, as duas sublistas ordenadas (left e right) s√£o combinadas pela fun√ß√£o auxiliar merge, que mescla ambas em uma √∫nica lista ordenada.

A fun√ß√£o merge recebe as duas sublistas (arr_1 e arr_2) e percorre-as utilizando dois √≠ndices (i e j). Ela compara os elementos das duas sublistas e adiciona o menor elemento ao vetor de resultado (result) em cada passo, at√© que todos os elementos de uma das listas sejam adicionados. Depois disso, os elementos restantes da outra sublista s√£o adicionados a result.

A complexidade de tempo do recursive_merge_sort √© O(n log n) tanto no caso m√©dio quanto no pior caso, onde n √© o n√∫mero de elementos no vetor. Isso ocorre porque a fun√ß√£o divide o vetor repetidamente em duas metades, realizando compara√ß√µes e mesclagens, cada etapa exigindo O(n) opera√ß√µes.

#### 2.4.3.3 An√°lise de Resultados

A tabela a seguir mostra os resultados de tempo de execu√ß√£o das implementa√ß√µes da ordena√ß√£o de mesclagem (mergesort) iterativa e recursiva com vetores de 1000, 10000 e 100000 elementos.

| Tamanho | Mergesort Iterativo | Mergesort Recursivo |
|:---:|:---:|:---:|
| 1000 | 463.17¬µs | 550.43¬µs |
| 10000 | 4.14ms | 5.16ms |
| 100000 | 40.57ms | 49.43ms |

Em todas as situa√ß√µes, a vers√£o iterativa √© mais r√°pida do que a recursiva, e essa diferen√ßa se acentua conforme o tamanho do vetor aumenta. Para um vetor de 1000 elementos, o mergesort iterativo leva 463.17 ¬µs, enquanto o recursivo demora 550.43 ¬µs. Com um vetor de 10000 elementos, o iterativo executa em 4.14 ms e o recursivo em 5.16 ms. J√° para 100000 elementos, o iterativo leva 40.57 ms, enquanto o recursivo demora 49.43 ms.

Esses resultados indicam que o mergesort iterativo √© consistentemente mais eficiente do que o recursivo, principalmente para vetores maiores. Embora ambos possuam complexidade de tempo O(n log n), a implementa√ß√£o recursiva exige mais recursos de mem√≥ria de pilha devido √†s chamadas recursivas, o que resulta em maior sobrecarga de execu√ß√£o. Em contraste, a vers√£o iterativa evita essas chamadas e, consequentemente, √© mais r√°pida e eficiente em termos de uso de mem√≥ria.

Conclui-se que, embora a vers√£o recursiva do mergesort seja conceitualmente mais pr√≥xima do design do algoritmo, a vers√£o iterativa oferece desempenho superior, especialmente para grandes conjuntos de dados. Para listas menores, a diferen√ßa √© menos significativa, mas a vers√£o iterativa ainda se mostra vantajosa em termos de efici√™ncia.

### 2.4.4 Compara√ß√£o entre Algoritmos

Entre os algoritmos analisados, quicksort e mergesort s√£o muito mais eficientes que o bubble sort, especialmente com vetores maiores. Ambos possuem complexidade O(n log n), o que os torna adequados para grandes conjuntos de dados. O bubble sort, por outro lado, com complexidade O(n¬≤), √© vi√°vel apenas para listas muito pequenas.

Em todos os algoritmos, as vers√µes iterativas foram consistentemente mais r√°pidas que as recursivas, principalmente em vetores grandes. As vers√µes recursivas consomem mais mem√≥ria de pilha e enfrentam sobrecarga de chamada de fun√ß√£o, resultando em tempos de execu√ß√£o ligeiramente maiores. Essa diferen√ßa √© menos vis√≠vel em vetores menores, mas se intensifica em conjuntos maiores.

Para cen√°rios onde a efici√™ncia e o uso de mem√≥ria s√£o essenciais, quicksort e mergesort iterativos s√£o as melhores escolhas. O bubble sort, devido √† sua inefici√™ncia, deve ser evitado em casos pr√°ticos de ordena√ß√£o de grandes conjuntos de dados.

Dessa forma, os resultados mostram que o quicksort e o mergesort (principalmente as vers√µes iterativas) s√£o altamente eficazes para ordenar grandes vetores, enquanto o bubble sort √© impratic√°vel para a maioria dos casos.

